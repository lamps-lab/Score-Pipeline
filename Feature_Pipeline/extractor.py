"""
Object models for the Feature Pipeline to generate features for the DARPA SCORE project
----------Feature extraction and pre-processing for the Synthetic Prediction Market----------
"""

from Feature_Pipeline.models import Paper, Author, Organization, Address, Citation
from fuzzywuzzy import process
from Feature_Pipeline.utilities import elem_to_text
from bs4 import BeautifulSoup
from Feature_Pipeline.ack_pairs import *
from Feature_Pipeline.elsevier_api import getsemantic
import pickle
import textstat
from textblob import TextBlob
from allennlp.predictors.predictor import Predictor
from Feature_Pipeline.scripts.coCitation import coCite


_author_ = "Arjun Menon, and Sai Ajay"
_copyright_ = "Copyright 2021, Penn State University"
_license_ = ""
_maintainer_ = "Sai Ajay"
_email_ = "svm6277@psu.edu"

# TODO: Initialize properly | Load NLP models at a single point
predictor = None
try:
    predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/"
                                    "sst-roberta-large-2020.06.08.tar.gz")
    print("--- Sentiment Roberta Model Loaded Successfully ---")
except Exception as e:
    print(e)


# TODO: Ajay to document
class ReadPickle:
    def __init__(self, filename):
        with open(filename, 'rb') as handle:
            self.d = pickle.load(handle)

    def get_rank(self, university):
        t = process.extractOne(university, self.d.keys())
        # print('################################')
        # print(t)
        # print('################################')
        # print('################################')
        # print(self.d[t[0]])
        # print('################################')
        if t[1] > 95:
            return 1 - (self.d[t[0]]/101)
        else:
            return 2

    def get_sjr(self, journal):
        t = process.extractOne(journal, self.d['dc:title'].to_list())
        if t[1] > 95:
            return self.d.loc[self.d['dc:title'] == t[0]]['SJR'][0]


class TEIExtractor:
    """
    This is an extraction/parsing module designed to pull apart entities of interest from the TEI
    formatted XML document model generated by the GROBID PDF extraction process.
    """

    def __init__(self, file, doi_dict, title_dict, db=None):
        """
        :param file: The XML file that needs to be parsed
        :param db: Object of class Database to cache/store result for memoization - refer to database.py
        :param test_tsv: Metadata file - use meta-file DOI as opposed to GROBID extracted DOI
        # TODO generalize this parameter to make use of data available in metadata at the extractor level
        """
        self.file = file
        self.db = db
        self.document = doi_dict
        self.document2 = title_dict
        self.uni_rank = ReadPickle('/data/xwei/score_pileline_app/Feature_Pipeline/uni_rank.pickle')
        self.sjr = ReadPickle('/data/xwei/score_pileline_app/Feature_Pipeline/journal_dictionary.pkl')
        self.paper = Paper()
        with open(file, 'rb') as tei:
            self.soup = BeautifulSoup(tei, features="lxml")
        

    def extract_paper_info(self):
        """
        Pulls apart information from the XML and invokes relevant functions to
        set the paper object and compute SCORE features.
        """
        xmlname = self.file.split('/')[-1]    #Xin
        xmlname = xmlname.split('.')[0]        #Xin
        print('xml:', xmlname)                  #Xin
        print(type(xmlname))                   #Xin
        
        #print(self.document['18'])
        
        #self.paper.doi = self.document['18'] 
        #print(self.paper.doi)
        
        
        # DOI
        doi = self.soup.teiheader.find("idno", type="DOI")
       
        if self.document:                                     #Xin
            self.paper.doi = self.document[xmlname]             #Xin
        elif doi:
            self.paper.doi = elem_to_text(doi)
        print('doi:', self.paper.doi)
            
        # Title
        title = self.soup.teiheader.find("title")
        if title:
            self.paper.title = elem_to_text(title)
            print('title1:', self.paper.title)
            if len(self.paper.title) < 2:                     #Xin
                self.paper.title = self.document2[xmlname]     #Xin
                print('title3:', self.paper.title)             #Xin
        elif self.document2:                                   #Xin
            self.paper.title = self.document2[xmlname]          #Xin
            print('title2:', self.paper.title)                  #Xin
        
        # Authors
        # Invokes a helper function to set list of Author objects
        authors = self.get_authors(self.soup.analytic.find_all('author'))
        if authors:
            self.paper.authors = authors
        if self.soup.abstract:
            self.paper.abstract = elem_to_text(self.soup.abstract)
        # Year
        published = self.soup.analytic.find("publicationstmt")
        if published:
            self.paper.year = elem_to_text(published.find("date", type="when"))
        # Organization / Affiliations
        affiliations = self.soup.analytic.find_all('affiliation')
        for affiliation in affiliations:
            org = Organization()
            org.type = "institution"
            org.name = elem_to_text(affiliation.find("orgname", type="institution"))
            address = Address()
            addr = affiliation.find("address")
            if addr:
                address.place = elem_to_text(addr.find("settlement"))
                address.region = elem_to_text(addr.find("region"))
                address.country = elem_to_text(addr.find("country"))
            org.address = address
            self.paper.affiliations.append(org)
        # University Ranking | Compute the university rank feature based on either the
        # first available affiliation or the second when first isn't available
        if self.paper.affiliations:
            if self.paper.affiliations[0] != '':
                self.paper.uni_rank = self.uni_rank.get_rank(self.paper.affiliations[0].name)
            elif len(self.paper.affiliations) > 1:
                self.paper.uni_rank = self.uni_rank.get_rank(self.paper.affiliations[1].name)
        else:
            self.paper.uni_rank = self.uni_rank.get_rank('Random')
        # Citations
        bibliography = self.soup.listbibl.find_all('biblstruct')
        for bibl in bibliography:
            citation = Citation()
            cited_paper = bibl.analytic
            if cited_paper:
                citation.title = elem_to_text(cited_paper.find("title", type="main"))
                citation_authors = self.get_authors(cited_paper.find_all("author"))
                citation.doi = elem_to_text(cited_paper.find("idno", type="DOI"))
                if citation_authors:
                    citation.authors = citation_authors
            cited_journal = bibl.monogr
            if cited_journal:
                citation.source = elem_to_text(cited_journal.find("title"))
                try:
                    citation.publish_year = cited_journal.imprint.date['when']
                except TypeError:
                    pass
            self.paper.citations.append(citation)
        # NER - Ack pairs - Funding status
        self.paper.ack_pairs = self.get_funding_status()
        er_list = [org for (entity, org) in self.paper.ack_pairs]
        # Set to 1 if any one of the entities acknowledged is an organization | Assumed to imply funding
        if 'ORG' in er_list:
            self.paper.funded = 1
        else:
            self.paper.funded = 0
        # TODO: Rajal to document API based meta-features/features
        # SJR
        api_resp = self.get_sjr(self.paper.doi, self.paper.title, self.db)
        if api_resp:
            self.paper.cited_by_count = api_resp["num_citations"]
            self.paper.sjr = api_resp["sjr"]
            self.paper.subject = api_resp["subject"]
            self.paper.subject_code = api_resp["subject_code"]
            self.paper.normalized = api_resp["normalized_citations"]
            self.paper.velocity = api_resp["citationVelocity"]
            self.paper.influentialcitations = api_resp["influentialCitationCount"]
            self.paper.references = api_resp["references_count"]
            self.paper.flag = api_resp["openaccessflag"]
            self.paper.influentialref = api_resp["influentialReferencesCount"]
            self.paper.ref_background = api_resp["reference_background"]
            self.paper.ref_result = api_resp["reference_result"]
            self.paper.ref_method = api_resp["reference_methodology"]
            self.paper.cite_background = api_resp["citations_background"]
            self.paper.cite_result = api_resp["citations_result"]
            self.paper.cite_method = api_resp["citations_methodology"]
            self.paper.cite_next = api_resp["citations_next"]
            self.paper.influential_references_methodology = api_resp["upstream_influential_methodology_count"]
            self.paper.issn = api_resp["ISSN"]
            self.paper.auth = api_resp["authors"]
            self.paper.age = api_resp["age"]
            if api_resp["abstract"]:
                self.paper.abstract = api_resp["abstract"]
        # Set self-citations
        self.paper.self_citations = self.paper.set_self_citations()
        # TODO: Xin/Ajay to document?
        # calculate coCitations
        t2, t3 = coCite(self.paper.doi, self.db)
        # TODO: Ajay to document
        # Calculate NLP features
        reading_score = self.get_reading_score(self.paper.abstract)
        subjectivity = self.get_subjectivity(self.paper.abstract)
        sentiment = self.get_sentiment(self.paper.abstract)
        result = {"doi": self.paper.doi, "title": self.paper.title, "num_citations": self.paper.cited_by_count,
                  "author_count": len(self.paper.authors), "sjr": self.paper.sjr, "u_rank": self.paper.uni_rank,
                  "funded": self.paper.funded, "self_citations": self.paper.self_citations,
                  "subject": self.paper.subject,"subject_code": self.paper.subject_code, "authors": self.paper.auth,
                  "citationVelocity": self.paper.velocity, "citations": api_resp["citations"],
                  "references_count": self.paper.references, "openaccessflag": self.paper.flag, "age": self.paper.age,
                  "influentialReferencesCount": self.paper.influentialref, "ISSN": self.paper.issn,
                  "normalized_citations": self.paper.normalized, "reference_background": self.paper.ref_background,
                  "reference_result": self.paper.ref_result, "reference_methodology": self.paper.ref_method,
                  "citations_background": self.paper.cite_background, "citations_result": self.paper.cite_result,
                  "citations_methodology": self.paper.cite_method, "citations_next": self.paper.cite_next,
                  "upstream_influential_methodology_count": self.paper.influential_references_methodology,
                  "coCite2": t2, "coCite3": t3, "influentialCitationCount": self.paper.influentialcitations,
                  "reading_score": reading_score, "subjectivity": subjectivity, "sentiment": sentiment}
        return result

    # TODO: Ajay to document reading score, subjectivity, and sentiment
    def get_reading_score(self, abstract):
        if isinstance(abstract, str):
            if not abstract: return 0
            return textstat.flesch_reading_ease(abstract)
        return 0

    def get_subjectivity(self, abstract):
        if isinstance(abstract,str):
            if len(abstract) < 10:return -1
            txtblob = TextBlob(abstract)
            return txtblob.sentiment.subjectivity
        return -1

    def get_sentiment(self, abstract):
        if isinstance(abstract, str):
            if len(abstract) < 10:
                return -1
            elif len(abstract) < 1690:
                label = predictor.predict(abstract)['label']
            else:
                abstract = abstract[0:1700]
                label = predictor.predict(abstract)['label']
            # abstract = abstract[0:1700]
            # print('#######################################################################')
            # print(abstract)
            # print(len(abstract))
            # print(type(abstract))
            # print('#######################################################################')
            # print('#######################################################################')
            # print(label)1156
            # print('#######################################################################')
            # label = predictor.predict(abstract)['label']
            # print('#######################################################################')
            # print(predictor.predict(abstract))
            # print('#######################################################################')
            return int(label)
        return -1

    @staticmethod
    def get_authors(authors):
        """
        Extract authors from XML subtree and initialize distinct objects of class Author for each author of the paper

        :param authors: Subtree in XML containing elements of entity type <author>
        :return: List of Objects of type Author -> refer to models.py
        """
        authors_list = []
        for author in authors:
            person = Author()
            pers_name = author.persname
            # Case where the entity is incomplete/does not have required information
            if not pers_name:
                continue
            person.first_name = elem_to_text(pers_name.find("forename", type="first"))
            person.middle_name = elem_to_text(pers_name.find("forename", type="middle"))
            person.surname = elem_to_text(pers_name.surname)
            person.set_name()
            if not any(auth.name == person.name for auth in authors_list):
                authors_list.append(person)
        return authors_list

    def get_funding_status(self):
        """
        Retrieves Entity:EntityType from relevant acknowledgement text from the paper
        :return: [(Entity:EntityType] -> List of tuples of strings
        """
        pairs = NER(XML2ack(self.file))
        return pairs

    # TODO: Rajal to document
    @staticmethod
    def get_sjr(doi, title, db):
        response = getsemantic(doi,title,db)
        crossref = response.get_row()
        scopus_search = response.return_search()
        serial_title = response.return_serialtitle()
        semantic = response.return_semantic()
        # pdb.set_trace()
        final = {"doi": response.doi, "title": response.title, "sjr": response.sjr, "num_citations": response.citedby,
                 "subject": response.subject, "subject_code": response.subject_code,
                 "normalized_citations": response.normalized, "citationVelocity": response.velocity,
                 "influentialCitationCount": response.incite, "references_count": response.refcount,
                 "openaccessflag": response.openaccess, "influentialReferencesCount": response.inref,
                 "reference_background": response.refback, "reference_result": response.refresult,
                 "reference_methodology": response.refmeth, "citations_background": response.cback,
                 "citations_result": response.cresult, "citations_methodology": response.cmeth,
                 "citations_next": response.next, "ISSN": response.issn, "authors": response.auth,
                 "upstream_influential_methodology_count": response.upstream_influential_methodology_count,
                 "citations": response.citations, "age": response.age, "abstract": response.ab}
        return final


# To test extractor response, run as main
if __name__ == "__main__":

    # TODO: update the path to journal_dict.csv. All configurable parameters need to go into the app_config.yaml
    #  and be read or passed down from a calling script
    test = r"C:\Users\arjun\dev\GROBID_processed\ReplicationProject\TRUE\0pxro.tei.xml"
    extractor = TEIExtractor(test)
    test_paper = extractor.extract_paper_info()
    print(test_paper)
